Mi primera impresión es que el problema es igual pero muchísimo más grande en tamaño. 
Mi primer intento será usar el mismo código, tomar nota de la diferencia en performance y trabajar a partir de ello.
Una observación es que correr al correr el programa varias veces manualmente se encuentra
el mejor tiempo del primer problema (61) mucho más frecuente que al encerrar el programa en un loop que lo repite.
Además es más rápido con cada iteración subsecuente, por lo cual no me sirve para benchmarking ni para mejorar el resultado.
El algoritmo que aproxima el resultado, de la entrega 1, es extremadamente rápido tardando 0.055s en promedio para 385 prendas, 
contra 0.0152s para 20 prendas. Esto me hace pensar que la gran parte del tiempo de ejecución es la "preparación" del programa.
Escribi un programa de batch que ejecuta el programa N veces independientemente (para evitar el problema explicado en la entrega 1)
y esto me permitió encontrar el tiempo 457 (una mejora de casi 40 unidades de tiempo respecto de mi primera ejecución).
Al ver el ranking, veo que hay 3 personas con mucho mejores tiempos. 
Intentaré codear un programa de backtracking para encontrar el óptimo. 
Tras multiples horas intentando adaptar mi resolucion a backtracking, fallé. 
Intentaré la misma idea que en main pero con un elemento de azar al elegir la prenda más lenta.
Tras otras miles de ejecuciones con la version de azar, logré reducir a 450 el tiempo total.
Si no fuese porque hay 3 personas con mejor tiempo pensaría que estoy cerca del óptimo, pero 
claramente mi algoritmo está lejos de aproximar el óptimo bien pues como mucho llegó a 450.
Si fuese un buen algoritmo eventualmente produciría cualquier valor entre 273 y 450, pero jamás salen.
