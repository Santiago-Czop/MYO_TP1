Mi primera impresión es que el problema es igual pero muchísimo más grande en tamaño. 
Mi primer intento será usar el mismo código, tomar nota de la diferencia en performance y trabajar a partir de ello.
Una observación es que correr al correr el programa varias veces manualmente se encuentra
el mejor tiempo del primer problema (61) mucho más frecuente que al encerrar el programa en un loop que lo repite.
Además es más rápido con cada iteración subsecuente, por lo cual no me sirve para benchmarking ni para mejorar el resultado.
El algoritmo que aproxima el resultado, de la entrega 1, es extremadamente rápido tardando 0.055s en promedio para 385 prendas, 
contra 0.0152s para 20 prendas. Esto me hace pensar que la gran parte del tiempo de ejecución es la "preparación" del programa.
Escribi un programa de batch que ejecuta el programa N veces independientemente (para evitar el problema explicado en la entrega 1)
y esto me permitió encontrar el tiempo 457 (una mejora de casi 40 unidades de tiempo respecto de mi primera ejecución).
Al ver el ranking, veo que hay 3 personas con mucho mejores tiempos. 
Intentaré codear un programa de backtracking para encontrar el óptimo. 
Tras multiples horas intentando adaptar mi resolucion a backtracking, fallé. 
Intentaré la misma idea que en main pero con un elemento de azar al elegir la prenda más lenta.
Tras otras miles de ejecuciones con la version de azar, logré reducir a 450 el tiempo total.
Si no fuese porque hay 3 personas con mejor tiempo pensaría que estoy cerca del óptimo, pero 
claramente mi algoritmo está lejos de aproximar el óptimo bien pues como mucho llegó a 450.
Si fuese un buen algoritmo eventualmente produciría cualquier valor entre 273 y 450, pero jamás salen. 
No entiendo por qué. No se si será una cuestión probabilística.
Tuve una nueva idea. Supongamos que cada prenda que tarda el mismo tiempo puede lavarse junta. El tiempo total sería 210
siguiendo una ecuación de sumatoria de Gauss. Por lo tanto, sería buena idea intentar que en cada subset, haya tantas
prendas lentas como sea posible. ¿Cómo? Esto sería encontrar el clique más grande de tiempo 20, luego de 19 compatible con 
los de 20 elegidos, luego los de 18 compatibles con 19 y 20 elegidos, etc. Intentaré implementar algo así.
No tendré garantía de que el tiempo será mejor. Encontre este algoritmo online: Bron–Kerbosch Algorithm
