Mi primera impresión es que el problema es igual pero muchísimo más grande en tamaño. 
Mi primer intento será usar el mismo código, tomar nota de la diferencia en performance y trabajar a partir de ello.
Una observación es que correr al correr el programa varias veces manualmente se encuentra
el mejor tiempo del primer problema (61) mucho más frecuente que al encerrar el programa en un loop que lo repite.
Además es más rápido con cada iteración subsecuente, por lo cual no me sirve para benchmarking ni para mejorar el resultado.
El algoritmo que aproxima el resultado, de la entrega 1, es extremadamente rápido tardando 0.055s en promedio para 385 prendas, 
contra 0.0152s para 20 prendas. Esto me hace pensar que la gran parte del tiempo de ejecución es la "preparación" del programa.
Escribi un programa de batch que ejecuta el programa N veces independientemente (para evitar el problema explicado en la entrega 1)
y esto me permitió encontrar el tiempo 457 (una mejora de casi 40 unidades de tiempo respecto de mi primera ejecución).
Al ver el ranking, veo que hay 3 personas con mucho mejores tiempos. 
Intentaré codear un programa de backtracking para encontrar el óptimo. 
Tras multiples horas intentando adaptar mi resolucion a backtracking, fallé. 
Intentaré la misma idea que en main pero con un elemento de azar al elegir la prenda más lenta. (main_random.py)
Tras otras miles de ejecuciones con la version de azar, logré reducir a 450 el tiempo total.
Si no fuese porque hay 3 personas con mejor tiempo pensaría que estoy cerca del óptimo, pero 
claramente mi algoritmo está lejos de aproximar el óptimo bien pues como mucho llegó a 450.
Si fuese un buen algoritmo eventualmente produciría cualquier valor entre 273 y 450, pero jamás salen. 
No entiendo por qué. No se si será una cuestión probabilística.
Tuve una nueva idea. Supongamos que cada prenda que tarda el mismo tiempo puede lavarse junta. El tiempo total sería 210
siguiendo una ecuación de sumatoria de Gauss. Por lo tanto, sería buena idea intentar que en cada subset, haya tantas
prendas lentas como sea posible. ¿Cómo? Esto sería encontrar el clique más grande de tiempo 20, luego de 19 compatible con 
los de 20 elegidos, luego los de 18 compatibles con 19 y 20 elegidos, etc. Intentaré implementar algo así.
No tendré garantía de que el tiempo será mejor. Encontre este algoritmo online: Bron–Kerbosch Algorithm
Terminé el algoritmo (main_bronk.py), y dado que es exponencial es mucho más lento, 
y los resultados son muy similares pues se basa en la misma idea. Intentaré otras ideas y reportaré la mejor. 
La primer idea es en vez de arrancar en tiempo 20 para el clique base, arrancar en el clique más grande de tiempo n.
No puedo encontrar el clique más grande de todos pues mi compu no lo soporta pero si de cada tiempo.
La otra idea es aproximar el clique más grande del grafo entero de alguna forma.
Tras mucha investigación descubrí que el problema es el de división de un grafo en cliques, que es 
análogo al coloreo del complemento del grafo, que sería el grafo de incompatibilidad. Encontré una librería 
famosa que lo resuelve de forma greedy (similar a mi solución del ejercicio 1) con distintas estrategias.
Tras confirmar que dicha librería provee una estrategia de coloreo del grafo de incompatibilidad que devuelve 270 (main_greedy_v1.py),
llamado smallest_last, leí el paper en el cual se basa y me sorprendió que parece ser el mismo algoritmo que
había imaginado yo (llamado largest_first, según el paper, y que se considera la forma natural de pensar el coloreo
de un grafo) que resolvía el problema. La diferencia está en que tiene pequeñas refinaciones que le permiten ser 
óptimo para ciertos tipos de grafo. El algoritmo colorea último a los vértices con menor grado,
es decir, por último a los menos incompatibles. Además para calcular el grado de cada vértice, no considera los
que ya les puso orden para colorear (primero se ordenan los vértices, luego se colorean). Esto es lo que causa
que haya diferencia entre largest_first y smallest_last, pues serían iguales si no disminuimos el grado de los adyacentes
al tomar un vértice. Lo implementaré yo mismo (main_greedy_v2.py), para aprender, por completitud y confirmación.
El último detalle es que la librería permite intercambiar los colores mientras colorea para ver si consigue
un color disponible, también descripto en el paper. Esto le garantiza 14 colores (lavados) siempre.